{
        "example3/README.md":
        "\n## Task 1\nIn `Unet3D`, accept distinct parameters for the size of the attention head for temporal and spatial attention and modify the definition of attention instances accordingly.\n\n## Task 2 \nCurrently the code has one error. The attention mechanism should be scaled to the root of the dimension of heads, $Attention(Q, K, V ) = softmax(\\frac{QK^T}{\\sqrt{\\text{dim\\_heads}}} )V$. But currently the code scales to just the number of heads, $Attention(Q, K, V ) = softmax(\\frac{QK^T}{\\text{dim\\_heads}} )V$. Solve this error."
,

        "example3/video_diffusion.py":
        "import math\nimport copy\nimport torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\nfrom functools import partial\n\nfrom torch.utils import data\nfrom pathlib import Path\nfrom torch.optim import Adam\nfrom torchvision import transforms as T, utils\nfrom torch.cuda.amp import autocast, GradScaler\nfrom PIL import Image\n\nfrom tqdm import tqdm\nfrom einops import rearrange\nfrom einops_exts import check_shape, rearrange_many\n\nfrom rotary_embedding_torch import RotaryEmbedding\n\nfrom text import tokenize, bert_embed, BERT_MODEL_DIM\n\n# helpers functions\n\ndef exists(x):\n    return x is not None\n\ndef noop(*args, **kwargs):\n    pass\n\ndef is_odd(n):\n    return (n % 2) == 1\n\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if callable(d) else d\n\ndef cycle(dl):\n    while True:\n        for data in dl:\n            yield data\n\ndef num_to_groups(num, divisor):\n    groups = num // divisor\n    remainder = num % divisor\n    arr = [divisor] * groups\n    if remainder > 0:\n        arr.append(remainder)\n    return arr\n\ndef prob_mask_like(shape, prob, device):\n    if prob == 1:\n        return torch.ones(shape, device = device, dtype = torch.bool)\n    elif prob == 0:\n        return torch.zeros(shape, device = device, dtype = torch.bool)\n    else:\n        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n\ndef is_list_str(x):\n    if not isinstance(x, (list, tuple)):\n        return False\n    return all([type(el) == str for el in x])\n\n# relative positional bias\n\nclass RelativePositionBias(nn.Module):\n    def __init__(\n        self,\n        heads = 8,\n        num_buckets = 32,\n        max_distance = 128\n    ):\n        super().__init__()\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n\n    @staticmethod\n    def _relative_position_bucket(relative_position, num_buckets = 32, max_distance = 128):\n        ret = 0\n        n = -relative_position\n\n        num_buckets //= 2\n        ret += (n < 0).long() * num_buckets\n        n = torch.abs(n)\n\n        max_exact = num_buckets // 2\n        is_small = n < max_exact\n\n        val_if_large = max_exact + (\n            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n        ).long()\n        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n\n        ret += torch.where(is_small, n, val_if_large)\n        return ret\n\n    def forward(self, n, device):\n        q_pos = torch.arange(n, dtype = torch.long, device = device)\n        k_pos = torch.arange(n, dtype = torch.long, device = device)\n        rel_pos = rearrange(k_pos, 'j -> 1 j') - rearrange(q_pos, 'i -> i 1')\n        rp_bucket = self._relative_position_bucket(rel_pos, num_buckets = self.num_buckets, max_distance = self.max_distance)\n        values = self.relative_attention_bias(rp_bucket)\n        return rearrange(values, 'i j h -> h i j')\n\n# small helper modules\n\nclass EMA():\n    def __init__(self, beta):\n        super().__init__()\n        self.beta = beta\n\n    def update_model_average(self, ma_model, current_model):\n        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n            old_weight, up_weight = ma_params.data, current_params.data\n            ma_params.data = self.update_average(old_weight, up_weight)\n\n    def update_average(self, old, new):\n        if old is None:\n            return new\n        return old * self.beta + (1 - self.beta) * new\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, *args, **kwargs):\n        return self.fn(x, *args, **kwargs) + x\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = x[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n\ndef Upsample(dim):\n    return nn.ConvTranspose3d(dim, dim, (1, 4, 4), (1, 2, 2), (0, 1, 1))\n\ndef Downsample(dim):\n    return nn.Conv3d(dim, dim, (1, 4, 4), (1, 2, 2), (0, 1, 1))\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dim, eps = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.gamma = nn.Parameter(torch.ones(1, dim, 1, 1, 1))\n\n    def forward(self, x):\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) / (var + self.eps).sqrt() * self.gamma\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = LayerNorm(dim)\n\n    def forward(self, x, **kwargs):\n        x = self.norm(x)\n        return self.fn(x, **kwargs)\n\n# building block modules\n\n\nclass Block(nn.Module):\n    def __init__(self, dim, dim_out, groups = 8):\n        super().__init__()\n        self.proj = nn.Conv3d(dim, dim_out, (1, 3, 3), padding = (0, 1, 1))\n        self.norm = nn.GroupNorm(groups, dim_out)\n        self.act = nn.SiLU()\n\n    def forward(self, x, scale_shift = None):\n        x = self.proj(x)\n        x = self.norm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        return self.act(x)\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, dim_out, *, time_emb_dim = None, groups = 8):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(time_emb_dim, dim_out * 2)\n        ) if exists(time_emb_dim) else None\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n        self.res_conv = nn.Conv3d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n\n    def forward(self, x, time_emb = None):\n\n        scale_shift = None\n        if exists(self.mlp):\n            assert exists(time_emb), 'time emb must be passed in'\n            time_emb = self.mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -> b c 1 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x, scale_shift = scale_shift)\n\n        h = self.block2(h)\n        return h + self.res_conv(x)\n\nclass SpatialLinearAttention(nn.Module):\n    def __init__(self, dim, heads = 4, dim_head = 32):\n        super().__init__()\n        self.scale = 1 / dim_head\n        self.heads = heads\n        hidden_dim = dim_head * heads\n        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n\n    def forward(self, x):\n        b, c, f, h, w = x.shape\n        x = rearrange(x, 'b c f h w -> (b f) c h w')\n\n        qkv = self.to_qkv(x).chunk(3, dim = 1)\n        q, k, v = rearrange_many(qkv, 'b (h c) x y -> b h c (x y)', h = self.heads)\n\n        q = q.softmax(dim = -2)\n        k = k.softmax(dim = -1)\n\n        q = q * self.scale\n        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n\n        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w)\n        out = self.to_out(out)\n        return rearrange(out, '(b f) c h w -> b c f h w', b = b)\n\n# attention along space and time\n\nclass EinopsToAndFrom(nn.Module):\n    def __init__(self, from_einops, to_einops, fn):\n        super().__init__()\n        self.from_einops = from_einops\n        self.to_einops = to_einops\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        shape = x.shape\n        reconstitute_kwargs = dict(tuple(zip(self.from_einops.split(' '), shape)))\n        x = rearrange(x, f'{self.from_einops} -> {self.to_einops}')\n        x = self.fn(x, **kwargs)\n        x = rearrange(x, f'{self.to_einops} -> {self.from_einops}', **reconstitute_kwargs)\n        return x\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        heads = 4,\n        dim_head = 32,\n        rotary_emb = None\n    ):\n        super().__init__()\n        self.scale = 1 / dim_head\n        self.heads = heads\n        hidden_dim = dim_head * heads\n\n        self.rotary_emb = rotary_emb\n        self.to_qkv = nn.Linear(dim, hidden_dim * 3, bias = False)\n        self.to_out = nn.Linear(hidden_dim, dim, bias = False)\n\n    def forward(\n        self,\n        x,\n        pos_bias = None,\n        focus_present_mask = None\n    ):\n        n, device = x.shape[-2], x.device\n\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n\n        if exists(focus_present_mask) and focus_present_mask.all():\n            # if all batch samples are focusing on present\n            # it would be equivalent to passing that token's values through to the output\n            values = qkv[-1]\n            return self.to_out(values)\n\n        # split out heads\n\n        q, k, v = rearrange_many(qkv, '... n (h d) -> ... h n d', h = self.heads)\n\n        # scale\n\n        q = q * self.scale\n\n        # rotate positions into queries and keys for time attention\n\n        if exists(self.rotary_emb):\n            q = self.rotary_emb.rotate_queries_or_keys(q)\n            k = self.rotary_emb.rotate_queries_or_keys(k)\n\n        # similarity\n\n        sim = einsum('... h i d, ... h j d -> ... h i j', q, k)\n\n        # relative positional bias\n\n        if exists(pos_bias):\n            sim = sim + pos_bias\n\n        if exists(focus_present_mask) and not (~focus_present_mask).all():\n            attend_all_mask = torch.ones((n, n), device = device, dtype = torch.bool)\n            attend_self_mask = torch.eye(n, device = device, dtype = torch.bool)\n\n            mask = torch.where(\n                rearrange(focus_present_mask, 'b -> b 1 1 1 1'),\n                rearrange(attend_self_mask, 'i j -> 1 1 1 i j'),\n                rearrange(attend_all_mask, 'i j -> 1 1 1 i j'),\n            )\n\n            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n\n        # numerical stability\n\n        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n        attn = sim.softmax(dim = -1)\n\n        # aggregate values\n\n        out = einsum('... h i j, ... h j d -> ... h i d', attn, v)\n        out = rearrange(out, '... h n d -> ... n (h d)')\n        return self.to_out(out)\n\n# model\n\nclass Unet3D(nn.Module):\n    def __init__(\n        self,\n        dim,\n        cond_dim = None,\n        out_dim = None,\n        dim_mults=(1, 2, 4, 8),\n        channels = 3,\n        attn_heads = 8,\n        attn_dim_head = 32,\n        use_bert_text_cond = False,\n        init_dim = None,\n        init_kernel_size = 7,\n        use_sparse_linear_attn = True,\n        block_type = 'resnet',\n        resnet_groups = 8\n    ):\n        super().__init__()\n        self.channels = channels\n\n        # temporal attention and its relative positional encoding\n\n        rotary_emb = RotaryEmbedding(min(32, attn_dim_head))\n\n        temporal_attn = lambda dim: EinopsToAndFrom('b c f h w', 'b (h w) f c', Attention(dim, heads = attn_heads, dim_head = attn_dim_head, rotary_emb = rotary_emb))\n\n        self.time_rel_pos_bias = RelativePositionBias(heads = attn_heads, max_distance = 32) # realistically will not be able to generate that many frames of video... yet\n\n        # initial conv\n\n        init_dim = default(init_dim, dim)\n        assert is_odd(init_kernel_size)\n\n        init_padding = init_kernel_size // 2\n        self.init_conv = nn.Conv3d(channels, init_dim, (1, init_kernel_size, init_kernel_size), padding = (0, init_padding, init_padding))\n\n        self.init_temporal_attn = Residual(PreNorm(init_dim, temporal_attn(init_dim)))\n\n        # dimensions\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        # time conditioning\n\n        time_dim = dim * 4\n        self.time_mlp = nn.Sequential(\n            SinusoidalPosEmb(dim),\n            nn.Linear(dim, time_dim),\n            nn.GELU(),\n            nn.Linear(time_dim, time_dim)\n        )\n\n        # text conditioning\n\n        self.has_cond = exists(cond_dim) or use_bert_text_cond\n        cond_dim = BERT_MODEL_DIM if use_bert_text_cond else cond_dim\n\n        self.null_cond_emb = nn.Parameter(torch.randn(1, cond_dim)) if self.has_cond else None\n\n        cond_dim = time_dim + int(cond_dim or 0)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n\n        num_resolutions = len(in_out)\n\n        # block type\n\n        block_klass = partial(ResnetBlock, groups = resnet_groups)\n        block_klass_cond = partial(block_klass, time_emb_dim = cond_dim)\n\n        # modules for all layers\n\n        for ind, (dim_in, dim_out) in enumerate(in_out):\n            is_last = ind >= (num_resolutions - 1)\n\n            self.downs.append(nn.ModuleList([\n                block_klass_cond(dim_in, dim_out),\n                block_klass_cond(dim_out, dim_out),\n                Residual(PreNorm(dim_out, SpatialLinearAttention(dim_out, heads = attn_heads))) if use_sparse_linear_attn else nn.Identity(),\n                Residual(PreNorm(dim_out, temporal_attn(dim_out))),\n                Downsample(dim_out) if not is_last else nn.Identity()\n            ]))\n\n        mid_dim = dims[-1]\n        self.mid_block1 = block_klass_cond(mid_dim, mid_dim)\n\n        spatial_attn = EinopsToAndFrom('b c f h w', 'b f (h w) c', Attention(mid_dim, heads = attn_heads))\n\n        self.mid_spatial_attn = Residual(PreNorm(mid_dim, spatial_attn))\n        self.mid_temporal_attn = Residual(PreNorm(mid_dim, temporal_attn(mid_dim)))\n\n        self.mid_block2 = block_klass_cond(mid_dim, mid_dim)\n\n        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n            is_last = ind >= (num_resolutions - 1)\n\n            self.ups.append(nn.ModuleList([\n                block_klass_cond(dim_out * 2, dim_in),\n                block_klass_cond(dim_in, dim_in),\n                Residual(PreNorm(dim_in, SpatialLinearAttention(dim_in, heads = attn_heads))) if use_sparse_linear_attn else nn.Identity(),\n                Residual(PreNorm(dim_in, temporal_attn(dim_in))),\n                Upsample(dim_in) if not is_last else nn.Identity()\n            ]))\n\n        out_dim = default(out_dim, channels)\n        self.final_conv = nn.Sequential(\n            block_klass(dim * 2, dim),\n            nn.Conv3d(dim, out_dim, 1)\n        )\n\n    def forward_with_cond_scale(\n        self,\n        *args,\n        cond_scale = 2.,\n        **kwargs\n    ):\n        logits = self.forward(*args, null_cond_prob = 0., **kwargs)\n        if cond_scale == 1 or not self.has_cond:\n            return logits\n\n        null_logits = self.forward(*args, null_cond_prob = 1., **kwargs)\n        return null_logits + (logits - null_logits) * cond_scale\n\n    def forward(\n        self,\n        x,\n        time,\n        cond = None,\n        null_cond_prob = 0.,\n        focus_present_mask = None,\n        prob_focus_present = 0.  # probability at which a given batch sample will focus on the present (0. is all off, 1. is completely arrested attention across time)\n    ):\n        assert not (self.has_cond and not exists(cond)), 'cond must be passed in if cond_dim specified'\n        batch, device = x.shape[0], x.device\n\n        focus_present_mask = default(focus_present_mask, lambda: prob_mask_like((batch,), prob_focus_present, device = device))\n\n        time_rel_pos_bias = self.time_rel_pos_bias(x.shape[2], device = x.device)\n\n        x = self.init_conv(x)\n\n        x = self.init_temporal_attn(x, pos_bias = time_rel_pos_bias)\n\n        r = x.clone()\n\n        t = self.time_mlp(time) if exists(self.time_mlp) else None\n\n        # classifier free guidance\n\n        if self.has_cond:\n            batch, device = x.shape[0], x.device\n            mask = prob_mask_like((batch,), null_cond_prob, device = device)\n            cond = torch.where(rearrange(mask, 'b -> b 1'), self.null_cond_emb, cond)\n            t = torch.cat((t, cond), dim = -1)\n\n        h = []\n\n        for block1, block2, spatial_attn, temporal_attn, downsample in self.downs:\n            x = block1(x, t)\n            x = block2(x, t)\n            x = spatial_attn(x)\n            x = temporal_attn(x, pos_bias = time_rel_pos_bias, focus_present_mask = focus_present_mask)\n            h.append(x)\n            x = downsample(x)\n\n        x = self.mid_block1(x, t)\n        x = self.mid_spatial_attn(x)\n        x = self.mid_temporal_attn(x, pos_bias = time_rel_pos_bias, focus_present_mask = focus_present_mask)\n        x = self.mid_block2(x, t)\n\n        for block1, block2, spatial_attn, temporal_attn, upsample in self.ups:\n            x = torch.cat((x, h.pop()), dim = 1)\n            x = block1(x, t)\n            x = block2(x, t)\n            x = spatial_attn(x)\n            x = temporal_attn(x, pos_bias = time_rel_pos_bias, focus_present_mask = focus_present_mask)\n            x = upsample(x)\n\n        x = torch.cat((x, r), dim = 1)\n        return self.final_conv(x)\n\n# gaussian diffusion trainer class\n\ndef extract(a, t, x_shape):\n    b, *_ = t.shape\n    out = a.gather(-1, t)\n    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n\ndef cosine_beta_schedule(timesteps, s = 0.008):\n    \"\"\"\n    cosine schedule\n    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n    \"\"\"\n    steps = timesteps + 1\n    x = torch.linspace(0, timesteps, steps, dtype = torch.float64)\n    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return torch.clip(betas, 0, 0.9999)\n\nclass GaussianDiffusion(nn.Module):\n    def __init__(\n        self,\n        denoise_fn,\n        *,\n        image_size,\n        num_frames,\n        text_use_bert_cls = False,\n        channels = 3,\n        timesteps = 1000,\n        loss_type = 'l1',\n        use_dynamic_thres = False, # from the Imagen paper\n        dynamic_thres_percentile = 0.9\n    ):\n        super().__init__()\n        self.channels = channels\n        self.image_size = image_size\n        self.num_frames = num_frames\n        self.denoise_fn = denoise_fn\n\n        betas = cosine_beta_schedule(timesteps)\n\n        alphas = 1. - betas\n        alphas_cumprod = torch.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.loss_type = loss_type\n\n        # register buffer helper function that casts float64 to float32\n\n        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n\n        register_buffer('betas', betas)\n        register_buffer('alphas_cumprod', alphas_cumprod)\n        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n\n        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n\n        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n\n        register_buffer('posterior_variance', posterior_variance)\n\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n\n        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))\n        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n\n        # text conditioning parameters\n\n        self.text_use_bert_cls = text_use_bert_cls\n\n        # dynamic thresholding when sampling\n\n        self.use_dynamic_thres = use_dynamic_thres\n        self.dynamic_thres_percentile = dynamic_thres_percentile\n\n    def q_mean_variance(self, x_start, t):\n        mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n        variance = extract(1. - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool, cond = None, cond_scale = 1.):\n        x_recon = self.predict_start_from_noise(x, t=t, noise = self.denoise_fn.forward_with_cond_scale(x, t, cond = cond, cond_scale = cond_scale))\n\n        if clip_denoised:\n            s = 1.\n            if self.use_dynamic_thres:\n                s = torch.quantile(\n                    rearrange(x_recon, 'b ... -> b (...)').abs(),\n                    self.dynamic_thres_percentile,\n                    dim = -1\n                )\n\n                s.clamp_(min = 1.)\n                s = s.view(-1, *((1,) * (x_recon.ndim - 1)))\n\n            # clip by threshold, depending on whether static or dynamic\n            x_recon = x_recon.clamp(-s, s) / s\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.inference_mode()\n    def p_sample(self, x, t, cond = None, cond_scale = 1., clip_denoised = True):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x = x, t = t, clip_denoised = clip_denoised, cond = cond, cond_scale = cond_scale)\n        noise = torch.randn_like(x)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.inference_mode()\n    def p_sample_loop(self, shape, cond = None, cond_scale = 1.):\n        device = self.betas.device\n\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='sampling loop time step', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long), cond = cond, cond_scale = cond_scale)\n\n        return unnormalize_img(img)\n\n    @torch.inference_mode()\n    def sample(self, cond = None, cond_scale = 1., batch_size = 16):\n        device = next(self.denoise_fn.parameters()).device\n\n        if is_list_str(cond):\n            cond = bert_embed(tokenize(cond)).to(device)\n\n        batch_size = cond.shape[0] if exists(cond) else batch_size\n        image_size = self.image_size\n        channels = self.channels\n        num_frames = self.num_frames\n        return self.p_sample_loop((batch_size, channels, num_frames, image_size, image_size), cond = cond, cond_scale = cond_scale)\n\n    @torch.inference_mode()\n    def interpolate(self, x1, x2, t = None, lam = 0.5):\n        b, *_, device = *x1.shape, x1.device\n        t = default(t, self.num_timesteps - 1)\n\n        assert x1.shape == x2.shape\n\n        t_batched = torch.stack([torch.tensor(t, device=device)] * b)\n        xt1, xt2 = map(lambda x: self.q_sample(x, t=t_batched), (x1, x2))\n\n        img = (1 - lam) * xt1 + lam * xt2\n        for i in tqdm(reversed(range(0, t)), desc='interpolation sample time step', total=t):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long))\n\n        return img\n\n    def q_sample(self, x_start, t, noise = None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n\n    def p_losses(self, x_start, t, cond = None, noise = None, **kwargs):\n        b, c, f, h, w, device = *x_start.shape, x_start.device\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n\n        if is_list_str(cond):\n            cond = bert_embed(tokenize(cond), return_cls_repr = self.text_use_bert_cls)\n            cond = cond.to(device)\n\n        x_recon = self.denoise_fn(x_noisy, t, cond = cond, **kwargs)\n\n        if self.loss_type == 'l1':\n            loss = F.l1_loss(noise, x_recon)\n        elif self.loss_type == 'l2':\n            loss = F.mse_loss(noise, x_recon)\n        else:\n            raise NotImplementedError()\n\n        return loss\n\n    def forward(self, x, *args, **kwargs):\n        b, device, img_size, = x.shape[0], x.device, self.image_size\n        check_shape(x, 'b c f h w', c = self.channels, f = self.num_frames, h = img_size, w = img_size)\n        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n        x = normalize_img(x)\n        return self.p_losses(x, t, *args, **kwargs)\n\n# trainer class\n\nCHANNELS_TO_MODE = {\n    1 : 'L',\n    3 : 'RGB',\n    4 : 'RGBA'\n}\n\ndef seek_all_images(img, channels = 3):\n    assert channels in CHANNELS_TO_MODE, f'channels {channels} invalid'\n    mode = CHANNELS_TO_MODE[channels]\n\n    i = 0\n    while True:\n        try:\n            img.seek(i)\n            yield img.convert(mode)\n        except EOFError:\n            break\n        i += 1\n\n# tensor of shape (channels, frames, height, width) -> gif\n\ndef video_tensor_to_gif(tensor, path, duration = 120, loop = 0, optimize = True):\n    images = map(T.ToPILImage(), tensor.unbind(dim = 1))\n    first_img, *rest_imgs = images\n    first_img.save(path, save_all = True, append_images = rest_imgs, duration = duration, loop = loop, optimize = optimize)\n    return images\n\n# gif -> (channels, frame, height, width) tensor\n\ndef gif_to_tensor(path, channels = 3, transform = T.ToTensor()):\n    img = Image.open(path)\n    tensors = tuple(map(transform, seek_all_images(img, channels = channels)))\n    return torch.stack(tensors, dim = 1)\n\ndef identity(t, *args, **kwargs):\n    return t\n\ndef normalize_img(t):\n    return t * 2 - 1\n\ndef unnormalize_img(t):\n    return (t + 1) * 0.5\n\ndef cast_num_frames(t, *, frames):\n    f = t.shape[1]\n\n    if f == frames:\n        return t\n\n    if f > frames:\n        return t[:, :frames]\n\n    return F.pad(t, (0, 0, 0, 0, 0, frames - f))\n\nclass Dataset(data.Dataset):\n    def __init__(\n        self,\n        folder,\n        image_size,\n        channels = 3,\n        num_frames = 16,\n        horizontal_flip = False,\n        force_num_frames = True,\n        exts = ['gif']\n    ):\n        super().__init__()\n        self.folder = folder\n        self.image_size = image_size\n        self.channels = channels\n        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n\n        self.cast_num_frames_fn = partial(cast_num_frames, frames = num_frames) if force_num_frames else identity\n\n        self.transform = T.Compose([\n            T.Resize(image_size),\n            T.RandomHorizontalFlip() if horizontal_flip else T.Lambda(identity),\n            T.CenterCrop(image_size),\n            T.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, index):\n        path = self.paths[index]\n        tensor = gif_to_tensor(path, self.channels, transform = self.transform)\n        return self.cast_num_frames_fn(tensor)\n\n# trainer class\n\nclass Trainer(object):\n    def __init__(\n        self,\n        diffusion_model,\n        folder,\n        *,\n        ema_decay = 0.995,\n        num_frames = 16,\n        train_batch_size = 32,\n        train_lr = 1e-4,\n        train_num_steps = 100000,\n        gradient_accumulate_every = 2,\n        amp = False,\n        step_start_ema = 2000,\n        update_ema_every = 10,\n        save_and_sample_every = 1000,\n        results_folder = './results',\n        num_sample_rows = 4,\n        max_grad_norm = None\n    ):\n        super().__init__()\n        self.model = diffusion_model\n        self.ema = EMA(ema_decay)\n        self.ema_model = copy.deepcopy(self.model)\n        self.update_ema_every = update_ema_every\n\n        self.step_start_ema = step_start_ema\n        self.save_and_sample_every = save_and_sample_every\n\n        self.batch_size = train_batch_size\n        self.image_size = diffusion_model.image_size\n        self.gradient_accumulate_every = gradient_accumulate_every\n        self.train_num_steps = train_num_steps\n\n        image_size = diffusion_model.image_size\n        channels = diffusion_model.channels\n        num_frames = diffusion_model.num_frames\n\n        self.ds = Dataset(folder, image_size, channels = channels, num_frames = num_frames)\n\n        print(f'found {len(self.ds)} videos as gif files at {folder}')\n        assert len(self.ds) > 0, 'need to have at least 1 video to start training (although 1 is not great, try 100k)'\n\n        self.dl = cycle(data.DataLoader(self.ds, batch_size = train_batch_size, shuffle=True, pin_memory=True))\n        self.opt = Adam(diffusion_model.parameters(), lr = train_lr)\n\n        self.step = 0\n\n        self.amp = amp\n        self.scaler = GradScaler(enabled = amp)\n        self.max_grad_norm = max_grad_norm\n\n        self.num_sample_rows = num_sample_rows\n        self.results_folder = Path(results_folder)\n        self.results_folder.mkdir(exist_ok = True, parents = True)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.ema_model.load_state_dict(self.model.state_dict())\n\n    def step_ema(self):\n        if self.step < self.step_start_ema:\n            self.reset_parameters()\n            return\n        self.ema.update_model_average(self.ema_model, self.model)\n\n    def save(self, milestone):\n        data = {\n            'step': self.step,\n            'model': self.model.state_dict(),\n            'ema': self.ema_model.state_dict(),\n            'scaler': self.scaler.state_dict()\n        }\n        torch.save(data, str(self.results_folder / f'model-{milestone}.pt'))\n\n    def load(self, milestone, **kwargs):\n        if milestone == -1:\n            all_milestones = [int(p.stem.split('-')[-1]) for p in Path(self.results_folder).glob('**/*.pt')]\n            assert len(all_milestones) > 0, 'need to have at least one milestone to load from latest checkpoint (milestone == -1)'\n            milestone = max(all_milestones)\n\n        data = torch.load(str(self.results_folder / f'model-{milestone}.pt'))\n\n        self.step = data['step']\n        self.model.load_state_dict(data['model'], **kwargs)\n        self.ema_model.load_state_dict(data['ema'], **kwargs)\n        self.scaler.load_state_dict(data['scaler'])\n\n    def train(\n        self,\n        prob_focus_present = 0.,\n        focus_present_mask = None,\n        log_fn = noop\n    ):\n        assert callable(log_fn)\n\n        while self.step < self.train_num_steps:\n            for i in range(self.gradient_accumulate_every):\n                data = next(self.dl).cuda()\n\n                with autocast(enabled = self.amp):\n                    loss = self.model(\n                        data,\n                        prob_focus_present = prob_focus_present,\n                        focus_present_mask = focus_present_mask\n                    )\n\n                    self.scaler.scale(loss / self.gradient_accumulate_every).backward()\n\n                print(f'{self.step}: {loss.item()}')\n\n            log = {'loss': loss.item()}\n\n            if exists(self.max_grad_norm):\n                self.scaler.unscale_(self.opt)\n                nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n\n            self.scaler.step(self.opt)\n            self.scaler.update()\n            self.opt.zero_grad()\n\n            if self.step % self.update_ema_every == 0:\n                self.step_ema()\n\n            if self.step != 0 and self.step % self.save_and_sample_every == 0:\n                milestone = self.step // self.save_and_sample_every\n                num_samples = self.num_sample_rows ** 2\n                batches = num_to_groups(num_samples, self.batch_size)\n\n                all_videos_list = list(map(lambda n: self.ema_model.sample(batch_size=n), batches))\n                all_videos_list = torch.cat(all_videos_list, dim = 0)\n\n                all_videos_list = F.pad(all_videos_list, (2, 2, 2, 2))\n\n                one_gif = rearrange(all_videos_list, '(i j) c f h w -> c f (i h) (j w)', i = self.num_sample_rows)\n                video_path = str(self.results_folder / str(f'{milestone}.gif'))\n                video_tensor_to_gif(one_gif, video_path)\n                log = {**log, 'sample': video_path}\n                self.save(milestone)\n\n            log_fn(log)\n            self.step += 1\n\n        print('training completed')\n"
,

        "example3/text.py":
        "import torch\nfrom einops import rearrange\n\ndef exists(val):\n    return val is not None\n\n# singleton globals\n\nMODEL = None\nTOKENIZER = None\nBERT_MODEL_DIM = 768\n\ndef get_tokenizer():\n    global TOKENIZER\n    if not exists(TOKENIZER):\n        TOKENIZER = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n    return TOKENIZER\n\ndef get_bert():\n    global MODEL\n    if not exists(MODEL):\n        MODEL = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n        if torch.cuda.is_available():\n            MODEL = MODEL.cuda()\n\n    return MODEL\n\n# tokenize\n\ndef tokenize(texts, add_special_tokens = True):\n    if not isinstance(texts, (list, tuple)):\n        texts = [texts]\n\n    tokenizer = get_tokenizer()\n\n    encoding = tokenizer.batch_encode_plus(\n        texts,\n        add_special_tokens = add_special_tokens,\n        padding = True,\n        return_tensors = 'pt'\n    )\n\n    token_ids = encoding.input_ids\n    return token_ids\n\n# embedding function\n\n@torch.no_grad()\ndef bert_embed(\n    token_ids,\n    return_cls_repr = False,\n    eps = 1e-8,\n    pad_id = 0.\n):\n    model = get_bert()\n    mask = token_ids != pad_id\n\n    if torch.cuda.is_available():\n        token_ids = token_ids.cuda()\n        mask = mask.cuda()\n\n    outputs = model(\n        input_ids = token_ids,\n        attention_mask = mask,\n        output_hidden_states = True\n    )\n\n    hidden_state = outputs.hidden_states[-1]\n\n    if return_cls_repr:\n        return hidden_state[:, 0]               # return [cls] as representation\n\n    if not exists(mask):\n        return hidden_state.mean(dim = 1)\n\n    mask = mask[:, 1:]                          # mean all tokens excluding [cls], accounting for length\n    mask = rearrange(mask, 'b n -> b n 1')\n\n    numer = (hidden_state[:, 1:] * mask).sum(dim = 1)\n    denom = mask.sum(dim = 1)\n    masked_mean =  numer / (denom + eps)\n    return masked_mean\n"
}
